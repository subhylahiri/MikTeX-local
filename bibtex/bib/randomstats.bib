@article{Majumdar2009MPmaxdev,
abstract = {We present a Coulomb gas method to calculate analytically the probability of rare events where the maximum eigenvalue of a random matrix is much larger than its typical value. The large deviation function that characterizes this probability is computed explicitly for Wishart and Gaussian ensembles. The method is general and applies to other related problems, e.g., the joint large deviation function for large fluctuations of top eigenvalues. Our results are relevant to widely employed data compression techniques, namely, the principal components analysis. Analytical predictions are verified by extensive numerical simulations.},
primaryClass = {cond-mat.stat-mech},
archivePrefix = {arXiv},
arxivId = {0811.2290},
author = {Majumdar, Satya N and Vergassola, Massimo},
doi = {10.1103/PhysRevLett.102.060601},
eprint = {0811.2290},
file = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Majumdar, Vergassola - 2009 - Large deviations of the maximum eigenvalue for Wishart and Gaussian random matrices.pdf:pdf},
issn = {00319007},
journal = prl,
keywords = {Models,Principal Component Analysis,Principal Component Analysis: methods,Theoretical},
mendeley-groups = {Random matrices},
month = feb,
number = {6},
pages = {060601},
pmid = {19257572},
publisher = {American Physical Society},
title = {{Large deviations of the maximum eigenvalue for Wishart and Gaussian random matrices}},
url = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.102.060601},
volume = {102},
year = {2009}
}

@article{Vivo2007MPmindev,
abstract = {We compute analytically the probability of large fluctuations to the left of the mean of the largest eigenvalue in the Wishart (Laguerre) ensemble of positive definite random matrices. We show that the probability that all the eigenvalues of a (N x N) Wishart matrix W=X{\^{}}T X (where X is a rectangular M x N matrix with independent Gaussian entries) are smaller than the mean value {\textless}$\backslash$lambda{\textgreater}=N/c decreases for large N as {\$}\backslashsim \backslashexp[-\backslashfrac{\{}\backslashbeta{\}}{\{}2{\}}N{\^{}}2 \backslashPhi{\_}{\{}-{\}}(\backslashfrac{\{}2{\}}{\{}\backslashsqrt{\{}c{\}}{\}}+1;c)]{\$}, where $\backslash$beta=1,2 correspond respectively to real and complex Wishart matrices, c=N/M {\textless} 1 and $\backslash$Phi{\_}{\{}-{\}}(x;c) is a large deviation function that we compute explicitly. The result for the Anti-Wishart case (M {\textless} N) simply follows by exchanging M and N. We also analytically determine the average spectral density of an ensemble of constrained Wishart matrices whose eigenvalues are forced to be smaller than a fixed barrier. The numerical simulations are in excellent agreement with the analytical predictions.},
primaryClass = {cond-mat.stat-mech},
archivePrefix = {arXiv},
arxivId = {cond-mat/0701371},
author = {Vivo, Pierpaolo and Majumdar, Satya N and Bohigas, Oriol},
doi = {10.1088/1751-8113/40/16/005},
eprint = {0701371},
file = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Vivo, Majumdar, Bohigas - 2007 - Large deviations of the maximum eigenvalue in Wishart random matrices.pdf:pdf;:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Vivo, Majumdar, Bohigas - 2007 - Large deviations of the maximum eigenvalue in Wishart random matrices.pdf:pdf},
issn = {1751-8113},
journal = {J. Phys. A: Math. Theor.},
mendeley-groups = {Random matrices},
month = apr,
number = {16},
pages = {4317--4337},
primaryClass = {cond-mat},
title = {{Large deviations of the maximum eigenvalue in Wishart random matrices}},
url = {http://arxiv.org/abs/cond-mat/0701371},
volume = {40},
year = {2007}
}

@article{Katzav2010,
abstract = {We consider the large deviations of the smallest eigenvalue of the Wishart-Laguerre Ensemble. Using the Coulomb gas picture we obtain rate functions for the large fluctuations to the left and the right of the hard edge. Our results are compared with known exact results for $\beta$=1 finding good agreement. We also consider the case of almost square matrices finding new universal rate functions describing large fluctuations.},
primaryClass = {cond-mat.dis-nn},
archivePrefix = {arXiv},
arxivId = {1005.5058},
author = {Katzav, Eytan and {P{\'{e}}rez Castillo}, Isaac},
doi = {10.1103/PhysRevE.82.040104},
eprint = {1005.5058},
file = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Katzav, P{\'{e}}rez Castillo - 2010 - Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble.pdf:pdf;:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katzav, P{\'{e}}rez Castillo - 2010 - Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble.pdf:pdf},
issn = {1550-2376},
journal = pre,
mendeley-groups = {Random matrices},
month = oct,
number = {4 Pt 1},
pages = {040104},
pmid = {21230224},
publisher = {American Physical Society},
title = {{Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble}},
url = {http://journals.aps.org/pre/abstract/10.1103/PhysRevE.82.040104 http://arxiv.org/abs/1005.5058},
volume = {82},
year = {2010}
}

@article{Dasgupta2003JLlemma,
abstract = {A result of Johnson and Lindenstrauss [13] shows that a set of n points in high dimensional Euclidean space can be mapped into an O(log n/ϵ2)-dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 ± ϵ). In this note, we prove this theorem using elementary probabilistic techniques. {\textcopyright} 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60–65, 2002},
author = {Dasgupta, Sanjoy and Gupta, Anupam},
doi = {10.1002/rsa.10073},
file = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta, Gupta - 2003 - An Elementary Proof of a Theorem of Johnson and Lindenstrauss.pdf:pdf},
isbn = {1098-2418},
issn = {10429832},
journal = {Random Structures and Algorithms},
mendeley-groups = {Random matrices},
month = jan,
number = {1},
pages = {60--65},
title = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
url = {http://doi.wiley.com/10.1002/rsa.10073},
volume = {22},
year = {2003}
}

@article{Baraniuk2009JLmfld,
abstract = {We propose a new approach for nonadaptive dimensionality reduction$\backslash$nof manifold-modeled data, demonstrating that a small number of random$\backslash$nlinear projections can preserve key information about a manifold-modeled$\backslash$nsignal. We center our analysis on the effect of a random linear projection$\backslash$noperator ?: N ? M , M inf N, on a smooth well-conditioned K-dimensional$\backslash$nsubmanifold N . As our main theoretical contribution, we establish$\backslash$na sufficient number M of random projections to guarantee that, with$\backslash$nhigh probability, all pairwise Euclidean and geodesic distances between$\backslash$npoints on are well preserved under the mapping ?. Our results bear$\backslash$nstrong resemblance to the emerging theory of Compressed Sensing (CS),$\backslash$nin which sparse signals can be recovered from small numbers of random$\backslash$nlinear measurements. As in CS, the random measurements we propose$\backslash$ncan be used to recover the original data in N . Moreover, like the$\backslash$nfundamental bound in CS, our requisite M is linear in the "information$\backslash$nlevel" K and logarithmic in the ambient dimension N; we also identify$\backslash$na logarithmic dependence on the volume and conditioning of the manifold.$\backslash$nIn addition to recovering faithful approximations to manifold-modeled$\backslash$nsignals, however, the random projections we propose can also be used$\backslash$nto discern key properties about the manifold. We discuss connections$\backslash$nand contrasts with existing techniques in manifold learning, a setting$\backslash$nwhere dimensionality reducing mappings are typically nonlinear and$\backslash$nconstructed adaptively from a set of sampled training data.},
author = {Baraniuk, Richard G. and Wakin, Michael B.},
doi = {10.1007/s10208-007-9011-z},
file = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baraniuk, Wakin - 2009 - Random projections of smooth manifolds.pdf:pdf},
isbn = {0001406108},
issn = {16153375},
journal = {Foundations of Computational Mathematics},
keywords = {Compressed sensing,Dimensionality reduction,Johnson-Lindenstrauss lemma,Manifold learning,Manifolds,Random projections,Sparsity},
mendeley-groups = {Random matrices},
month = dec,
number = {1},
pages = {51--77},
title = {{Random projections of smooth manifolds}},
url = {http://link.springer.com/10.1007/s10208-007-9011-z},
volume = {9},
year = {2009}
}

@article{Johnson1984extensions,
author = {Johnson, William B. and Lindenstrauss, Joram},
doi = {10.1090/conm/026/737400},
isbn = {978-0-8218-5030-5 978-0-8218-7611-4},
journal = {Contemp. Math.},
mendeley-groups = {Random matrices},
number = {189-206},
pages = {189--206},
title = {{Extension of Lipschitz maps into a Hilbert space}},
url = {http://www.ams.org/conm/026/},
volume = {26},
year = {1984}
}

@article{Sarlos2006subspaceJL,
abstract = {Several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (lscr2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. 1) Independent of the results of Har-Peled and of Deshpande and Vempala, one of the first - and to the best of our knowledge the most efficient - relative error (1 + epsi) parA {\$}AkparF approximation algorithms for the singular value decomposition of an m times n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O((M(k/epsi+k log k) + (n+m)(k/epsi+k log k)2)log (1/sigma)). 2) The first o(nd2) time (1 + epsi) relative error approximation algorithm for n times d linear (lscr2) regression. 3) A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool},
author = {Sarlos, Tamas},
doi = {10.1109/FOCS.2006.37},
file = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Sarlos - 2006 - Improved Approximation Algorithms for Large Matrices via Random Projections.pdf:pdf},
isbn = {0-7695-2720-5},
issn = {0272-5428},
journal = {2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)},
keywords = {Algorithm design and analysis,Approximation algorithms,Automation,Boosting,Embedded computing,Linear algebra,Matrix decomposition,Sampling methods,Singular value decomposition,Sparse matrices,approximation theory,black box probability boosting tool,computational complexity,data dependent random sampling,large matrices,linear regression,matrix multiplication,norm approximation,random processes,random projections,regression analysis,relative error approximation,sampling methods,singular value decomposition},
mendeley-groups = {Random matrices},
pages = {143--152},
publisher = {IEEE},
shorttitle = {2006 47th Annual IEEE Symposium on Foundations of},
title = {{Improved Approximation Algorithms for Large Matrices via Random Projections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4031351 http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=4031351},
year = {2006}
}

@inproceedings{clarkson2008tighter,
 author = {Clarkson, Kenneth L.},
 title = {Tighter Bounds for Random Projections of Manifolds},
 booktitle = {Proc. 24th Annual Symp. on Computational Geometry},
 series = {SCG '08},
 year = {2008},
 isbn = {978-1-60558-071-5},
 location = {College Park, MD, USA},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1377676.1377685},
 doi = {10.1145/1377676.1377685},
 acmid = {1377685},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {johnson-lindenstrauss random projection}
}


@article{baraniuk2008simple,
author = {Baraniuk, Richard G and Davenport, M and DeVore, R and Wakin, M B},
journal = {Constructive Approximation},
mendeley-groups = {Random matrices},
number = {3},
pages = {253--263},
publisher = {Springer},
title = {{A Simple Proof of the Restricted Isometry Property for Random Matrices (aka the Johnson-Lindenstrauss lemma meets compressed sensing)}},
url = {http://www.dsp.ece.rice.edu/cs/JLCSfinalrevision1.pdf},
volume = {28},
year = {2007}
}

@article{blum2006random,
author = {Blum, Avrim},
doi = {10.1007/11752790_3},
isbn = {3-540-34137-4},
issn = {16113349},
journal = {Subspace, Latent Structure and Feature Selection},
mendeley-groups = {Random matrices},
pages = {52--68},
publisher = {Springer},
title = {{Random projection, margins, kernels, and feature-selection}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000238094700003},
volume = {3940},
year = {2006}
}

@article{davenport2007smashed,
abstract = {The theory of compressive sensing (CS) enables the reconstruction of a sparse or compressible image or signal from a small set of linear, non-adaptive (even random) projections. However, in many applications, including object and target recognition, we are ultimately interested in making a decision about an image rather than computing a reconstruction. We propose here a framework for compressive classification that operates directly on the compressive measurements without first reconstructing the image. We dub the resulting dimensionally reduced matched filter the smashed filter. The first part of the theory maps traditional maximum likelihood hypothesis testing into the compressive domain; we find that the number of measurements required for a given classification performance level does not depend on the sparsity or compressibility of the images but only on the noise level. The second part of the theory applies the generalized maximum likelihood method to deal with unknown transformations such as the translation, scale, or viewing angle of a target object. We exploit the fact the set of transformed images forms a low-dimensional, nonlinear manifold in the high-dimensional image space. We find that the number of measurements required for a given classification performance level grows linearly in the dimensionality of the manifold but only logarithmically in the number of pixels/samples and image classes. Using both simulations and measurements from a new single-pixel compressive camera, we demonstrate the effectiveness of the smashed filter for target classification using very few measurements.},
author = {Davenport, Mark A and Duarte, Marco F and Wakin, Michael B and Laska, Jason N and Takhar, Dharmpal and Kelly, Kevin F and Baraniuk, Richard},
doi = {10.1117/12.714460},
isbn = {0001406108},
issn = {0277786X},
journal = {Proc. SPIE 6498, Computational Imaging V},
keywords = {compressive sensing,image classification,object recognition,smashed filter},
mendeley-groups = {Random matrices},
month = jan,
pages = {64980H--1--12},
pmid = {21685898},
title = {{The smashed filter for compressive classification and target recognition}},
url = {http://dx.doi.org/10.1117/12.714460$\backslash$nhttp://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1298717},
volume = {6498},
year = {2007}
}

@inproceedings{duarte2006sparse,
author = {Duarte, Marco F and Davenport, Mark A and Wakin, Michael B and Baraniuk, Richard G},
booktitle = {Acoustics, Speech and Signal Processing, ICASSP Proceedings.},
doi = {10.1109/ICASSP.2006.1660651},
isbn = {1520-6149},
issn = {1520-6149},
mendeley-groups = {Random matrices},
organization = {IEEE},
pages = {305--308},
title = {{Sparse Signal Detection From Incoherent Projections}},
volume = {3},
year = {2006}
}

@inproceedings{duarte2007multiscale,
abstract = {We propose a framework for exploiting dimension-reducing random projections in detection and classification problems. Our approach is based on the generalized likelihood ratio test; in the case of image classification, it exploits the fact that a set of images of a fixed scene under varying articulation parameters forms a low-dimensional, nonlinear manifold. Exploiting recent results showing that random projections stably embed a smooth manifold in a lower-dimensional space, we develop the multiscale smashed filter as a compressive analog of the familiar matched filter classifier. In a practical target classification problem using a single-pixel camera that directly acquires compressive image projections, we achieve high classification rates using many fewer measurements than the dimensionality of the images.},
author = {Duarte, M F and Davenport, M A and Wakin, M B and Laska, J N and Takhar, D and Kelly, K F and Baraniuk, R G},
booktitle = {IEEE International Conference on Image Processing, 2007. ICIP 2007.},
doi = {10.1109/ICIP.2007.4379546},
isbn = {1522-4880},
keywords = {CS theory,Cameras,Classification algorithms,Image classification,Image coding,Image reconstruction,Instruments,Layout,Matched filters,Testing,Vectors,compressive image classification problem,data compression,filtering theory,generalized likelihood ratio test,image matching,multiscale dimension-reducing random projection,multiscale smashed filter classifier,object recognition,single-pixel camera,statistical testing},
mendeley-groups = {Random matrices},
organization = {IEEE},
pages = {161--164},
title = {{Multiscale Random Projections for Compressive Classification}},
volume = {6},
year = {2007}
}

@inproceedings{haupt2006compressive,
abstract = {Compressive sampling (CS), also called compressed sensing, entails making observations of an unknown signal by projecting it onto random vectors. Recent theoretical results show that if the signal is sparse (or nearly sparse) in some basis, then with high probability such observations essentially encode the salient information in the signal. Further, the signal can be reconstructed from these "random projections," even when the number of observations is far less than the ambient signal dimension. The provable success of CS for signal reconstruction motivates the study of its potential in other applications. This paper investigates the utility of CS projection observations for signal classification (more specifically, m-ary hypothesis testing). Theoretical error bounds are derived and verified with several simulations.},
author = {Haupt, Jarvis and Castro, Rui and Nowak, Robert and Fudge, Gerald and Yeh, Alex},
booktitle = {2006 Fortieth Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.2006.354994},
isbn = {1-4244-0784-2},
issn = {10586393},
mendeley-groups = {Random matrices},
organization = {IEEE},
pages = {1430--1434},
title = {{Compressive Sampling for Signal Classification}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=4176804{\&}contentType=Conference+Publications{\&}queryText=haupt+compressive+sensing},
year = {2006}
}

@inproceedings{hegde2007random,
abstract = {We propose a novel method for emphlinear dimensionality reduction of manifold modelled data. First we show that with a small number M of emphrandom projections of sample points in mathbbR N belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigourously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N, meaning that K {\textless} M ll N. To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.},
author = {Hegde, Chinmay and Wakin, Michael B},
booktitle = {Neural Information Processing Systems},
isbn = {9550071030},
mendeley-groups = {Random matrices},
pages = {641--648},
title = {{Random Projections for Manifold Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.2757{\&}rep=rep1{\&}type=pdf},
volume = {M},
year = {2007}
}
@inproceedings{indyk1998approximate,
author = {Indyk, P and Motwani, R},
booktitle = {Proceedings of the thirtieth annual ACM symposium on Theory of computing},
doi = {10.4086/toc.2012.v008a014},
isbn = {0897919629},
issn = {00123692},
mendeley-groups = {Random matrices},
organization = {ACM},
pages = {604--613},
pmid = {15486356},
title = {{Approximate nearest neighbors: towards removing the curse of dimensionality}},
year = {1998}
}
@article{zhou2009compressed,
abstract = {Recent research has studied the role of sparsity in high-dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models. This line of work shows that lscr1 -regularized least squares regression can accurately estimate a sparse linear model from noisy examples in high dimensions. We study a variant of this problem where the original n input variables are compressed by a random linear transformation to m Lt n examples in p dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of projections that are required for lscr1 -regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ldquosparsistence.rdquo We also show that lscr1 -regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ldquopersistence.rdquo Finally, we characterize the privacy properties of the compression procedure, establishing upper bounds on the mutual information between the compressed and uncompressed data that decay to zero.},
author = {Zhou, Shuheng and Lafferty, John and Wasserman, Larry},
doi = {10.1109/TIT.2008.2009605},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Trans. Inf. Theory},
keywords = {???1 regularization,Capacity of multiple-antenna channels,Compressed sensing,High-dimensional regression,Lasso,Privacy,Sparsity},
mendeley-groups = {Random matrices},
number = {2},
pages = {846--866},
publisher = {IEEE},
title = {{Compressed and privacy-sensitive sparse regression}},
volume = {55},
year = {2009}
}

@article{candes2005decoding,
abstract = {This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f∈Rn from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the ℓ1-minimization problem (||x||ℓ1:=$\Sigma$i|xi|) min(g∈Rn) ||y - Ag||ℓ1 provided that the support of the vector of errors is not too large, ||e||ℓ0:=|{\{}i:ei ≠ 0{\}}|≤$\rho${\textperiodcentered}m for some $\rho${\textgreater}0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of ℓ1 is a crucial property we call the uniform uncertainty principle that we shall describe in detail.},
archivePrefix = {arXiv},
arxivId = {math/0502327},
author = {Candes, Emmanuel. J. and Tao, Terence},
doi = {10.1109/TIT.2005.858979},
eprint = {0502327},
file = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Candes, Tao - 2005 - Decoding by Linear Programming.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Basis pursuit,Decoding,Equations,Error correction,Error correction codes,Gaussian processes,Gaussian random matrices,Gaussian random matrix,Information theory,Linear code,Linear programming,Mathematics,Sparse matrices,Vectors,basis pursuit,convex programming,decoding,decoding of (random) linear codes,duality in optimization,error correction codes,indeterminancy,linear code decoding,linear codes,linear programming,minimisation,minimization problem,natural error correcting problem,principal angles,random codes,restricted orthonormality,simple convex optimization problem,singular values of random matrices,sparse matrices,sparse solution,sparse solutions to underdetermined systems,uncertainty principle},
mendeley-groups = {Random matrices},
month = dec,
number = {12},
pages = {4203--4215},
primaryClass = {math.MG},
shorttitle = {IEEE Transactions on Information Theory},
title = {{Decoding by Linear Programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1542412 http://arxiv.org/abs/math/0502327},
volume = {51},
year = {2005}
}
