@article{Majumdar2009MPmaxdev,
  abstract     = {We present a Coulomb gas method to calculate analytically the probability of rare events where the maximum eigenvalue of a random matrix is much larger than its typical value. The large deviation function that characterizes this probability is computed explicitly for Wishart and Gaussian ensembles. The method is general and applies to other related problems, e.g., the joint large deviation function for large fluctuations of top eigenvalues. Our results are relevant to widely employed data compression techniques, namely, the principal components analysis. Analytical predictions are verified by extensive numerical simulations.},
  primaryclass = {cond-mat.stat-mech},
  archiveprefix= {arXiv},
  arxivid      = {0811.2290},
  author       = {Majumdar, Satya N and Vergassola, Massimo},
  doi          = {10.1103/PhysRevLett.102.060601},
  eprint       = {0811.2290},
  file         = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Majumdar, Vergassola - 2009 - Large deviations of the maximum eigenvalue for Wishart and Gaussian random matrices.pdf:pdf},
  issn         = {00319007},
  journal      = prl,
  keywords     = {Models,Principal Component Analysis,Principal Component Analysis: methods,Theoretical},
mendeley-groups = {Random matrices},
  month        = feb,
  number       = {6},
  pages        = {060601},
  pmid         = {19257572},
  publisher    = {American Physical Society},
  title        = {{Large deviations of the maximum eigenvalue for Wishart and Gaussian random matrices}},
  url          = {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.102.060601},
  volume       = {102},
  year         = {2009},
}

@article{Vivo2007MPmindev,
  abstract     = {We compute analytically the probability of large fluctuations to the left of the mean of the largest eigenvalue in the Wishart (Laguerre) ensemble of positive definite random matrices. We show that the probability that all the eigenvalues of a (N x N) Wishart matrix W=X{\^{}}T X (where X is a rectangular M x N matrix with independent Gaussian entries) are smaller than the mean value {\textless}$\backslash$lambda{\textgreater}=N/c decreases for large N as {\$}\backslashsim \backslashexp[-\backslashfrac{\{}\backslashbeta{\}}{\{}2{\}}N{\^{}}2 \backslashPhi{\_}{\{}-{\}}(\backslashfrac{\{}2{\}}{\{}\backslashsqrt{\{}c{\}}{\}}+1;c)]{\$}, where $\backslash$beta=1,2 correspond respectively to real and complex Wishart matrices, c=N/M {\textless} 1 and $\backslash$Phi{\_}{\{}-{\}}(x;c) is a large deviation function that we compute explicitly. The result for the Anti-Wishart case (M {\textless} N) simply follows by exchanging M and N. We also analytically determine the average spectral density of an ensemble of constrained Wishart matrices whose eigenvalues are forced to be smaller than a fixed barrier. The numerical simulations are in excellent agreement with the analytical predictions.},
  primaryclass = {cond-mat.stat-mech},
  archiveprefix= {arXiv},
  arxivid      = {cond-mat/0701371},
  author       = {Vivo, Pierpaolo and Majumdar, Satya N and Bohigas, Oriol},
  doi          = {10.1088/1751-8113/40/16/005},
  eprint       = {0701371},
  file         = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Vivo, Majumdar, Bohigas - 2007 - Large deviations of the maximum eigenvalue in Wishart random matrices.pdf:pdf;:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Vivo, Majumdar, Bohigas - 2007 - Large deviations of the maximum eigenvalue in Wishart random matrices.pdf:pdf},
  issn         = {1751-8113},
  journal      = {J. Phys. A: Math. Theor.},
mendeley-groups = {Random matrices},
  month        = apr,
  number       = {16},
  pages        = {4317--4337},
  primaryclass = {cond-mat},
  title        = {{Large deviations of the maximum eigenvalue in Wishart random matrices}},
  url          = {http://arxiv.org/abs/cond-mat/0701371},
  volume       = {40},
  year         = {2007},
}

@article{Katzav2010,
  abstract     = {We consider the large deviations of the smallest eigenvalue of the Wishart-Laguerre Ensemble. Using the Coulomb gas picture we obtain rate functions for the large fluctuations to the left and the right of the hard edge. Our results are compared with known exact results for $\beta$=1 finding good agreement. We also consider the case of almost square matrices finding new universal rate functions describing large fluctuations.},
  primaryclass = {cond-mat.dis-nn},
  archiveprefix= {arXiv},
  arxivid      = {1005.5058},
  author       = {Katzav, Eytan and {P{\'{e}}rez Castillo}, Isaac},
  doi          = {10.1103/PhysRevE.82.040104},
  eprint       = {1005.5058},
  file         = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Katzav, P{\'{e}}rez Castillo - 2010 - Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble.pdf:pdf;:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katzav, P{\'{e}}rez Castillo - 2010 - Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble.pdf:pdf},
  issn         = {1550-2376},
  journal      = pre,
mendeley-groups = {Random matrices},
  month        = oct,
  number       = {4 Pt 1},
  pages        = {040104},
  pmid         = {21230224},
  publisher    = {American Physical Society},
  title        = {{Large deviations of the smallest eigenvalue of the Wishart-Laguerre ensemble}},
  url          = {http://journals.aps.org/pre/abstract/10.1103/PhysRevE.82.040104 http://arxiv.org/abs/1005.5058},
  volume       = {82},
  year         = {2010},
}

@article{Dasgupta2003JLlemma,
  abstract     = {A result of Johnson and Lindenstrauss [13] shows that a set of n points in high dimensional Euclidean space can be mapped into an O(log n/ϵ2)-dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 ± ϵ). In this note, we prove this theorem using elementary probabilistic techniques. {\textcopyright} 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60–65, 2002},
  author       = {Dasgupta, Sanjoy and Gupta, Anupam},
  doi          = {10.1002/rsa.10073},
  file         = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta, Gupta - 2003 - An Elementary Proof of a Theorem of Johnson and Lindenstrauss.pdf:pdf},
  isbn         = {1098-2418},
  issn         = {10429832},
  journal      = {Random Structures and Algorithms},
mendeley-groups = {Random matrices},
  month        = jan,
  number       = {1},
  pages        = {60--65},
  title        = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
  url          = {http://doi.wiley.com/10.1002/rsa.10073},
  volume       = {22},
  year         = {2003},
}

@article{Baraniuk2009JLmfld,
  abstract     = {We propose a new approach for nonadaptive dimensionality reduction$\backslash$nof manifold-modeled data, demonstrating that a small number of random$\backslash$nlinear projections can preserve key information about a manifold-modeled$\backslash$nsignal. We center our analysis on the effect of a random linear projection$\backslash$noperator ?: N ? M , M inf N, on a smooth well-conditioned K-dimensional$\backslash$nsubmanifold N . As our main theoretical contribution, we establish$\backslash$na sufficient number M of random projections to guarantee that, with$\backslash$nhigh probability, all pairwise Euclidean and geodesic distances between$\backslash$npoints on are well preserved under the mapping ?. Our results bear$\backslash$nstrong resemblance to the emerging theory of Compressed Sensing (CS),$\backslash$nin which sparse signals can be recovered from small numbers of random$\backslash$nlinear measurements. As in CS, the random measurements we propose$\backslash$ncan be used to recover the original data in N . Moreover, like the$\backslash$nfundamental bound in CS, our requisite M is linear in the "information$\backslash$nlevel" K and logarithmic in the ambient dimension N; we also identify$\backslash$na logarithmic dependence on the volume and conditioning of the manifold.$\backslash$nIn addition to recovering faithful approximations to manifold-modeled$\backslash$nsignals, however, the random projections we propose can also be used$\backslash$nto discern key properties about the manifold. We discuss connections$\backslash$nand contrasts with existing techniques in manifold learning, a setting$\backslash$nwhere dimensionality reducing mappings are typically nonlinear and$\backslash$nconstructed adaptively from a set of sampled training data.},
  author       = {Baraniuk, Richard G. and Wakin, Michael B.},
  doi          = {10.1007/s10208-007-9011-z},
  file         = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baraniuk, Wakin - 2009 - Random projections of smooth manifolds.pdf:pdf},
  isbn         = {0001406108},
  issn         = {16153375},
  journal      = {Foundations of Computational Mathematics},
  keywords     = {Compressed sensing,Dimensionality reduction,Johnson-Lindenstrauss lemma,Manifold learning,Manifolds,Random projections,Sparsity},
mendeley-groups = {Random matrices},
  month        = dec,
  number       = {1},
  pages        = {51--77},
  title        = {{Random projections of smooth manifolds}},
  url          = {http://link.springer.com/10.1007/s10208-007-9011-z},
  volume       = {9},
  year         = {2009},
}

@article{Johnson1984extensions,
  author       = {Johnson, William B. and Lindenstrauss, Joram},
  doi          = {10.1090/conm/026/737400},
  isbn         = {978-0-8218-5030-5 978-0-8218-7611-4},
  journal      = {Contemp. Math.},
mendeley-groups = {Random matrices},
  number       = {189-206},
  pages        = {189--206},
  title        = {{Extension of Lipschitz maps into a Hilbert space}},
  url          = {http://www.ams.org/conm/026/},
  volume       = {26},
  year         = {1984},
}

@article{Sarlos2006subspaceJL,
  abstract     = {Several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (lscr2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. 1) Independent of the results of Har-Peled and of Deshpande and Vempala, one of the first - and to the best of our knowledge the most efficient - relative error (1 + epsi) parA {\$}AkparF approximation algorithms for the singular value decomposition of an m times n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O((M(k/epsi+k log k) + (n+m)(k/epsi+k log k)2)log (1/sigma)). 2,
) The first o(nd2) time (1 + epsi) relative error approximation algorithm for n times d linear (lscr2) regression. 3) A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool},
author = {Sarlos, Tamas},
doi = {10.1109/FOCS.2006.37},
file = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Sarlos - 2006 - Improved Approximation Algorithms for Large Matrices via Random Projections.pdf:pdf},
isbn = {0-7695-2720-5},
issn = {0272-5428},
journal = {2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)},
keywords = {Algorithm design and analysis,Approximation algorithms,Automation,Boosting,Embedded computing,Linear algebra,Matrix decomposition,Sampling methods,Singular value decomposition,Sparse matrices,approximation theory,black box probability boosting tool,computational complexity,data dependent random sampling,large matrices,linear regression,matrix multiplication,norm approximation,random processes,random projections,regression analysis,relative error approximation,sampling methods,singular value decomposition},
mendeley-groups = {Random matrices},
pages = {143--152},
publisher = {IEEE},
shorttitle = {2006 47th Annual IEEE Symposium on Foundations of},
title = {{Improved Approximation Algorithms for Large Matrices via Random Projections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4031351 http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=4031351},
year = {2006}
}

@inproceedings{clarkson2008tighter,
  author       = {Clarkson, Kenneth L.},
  title        = {Tighter Bounds for Random Projections of Manifolds},
  booktitle    = {Proc. 24th Annual Symp. on Computational Geometry},
  series       = {SCG '08},
  year         = {2008},
  isbn         = {978-1-60558-071-5},
  location     = {College Park, MD, USA},
  pages        = {39--48},
  numpages     = {10},
  url          = {http://doi.acm.org/10.1145/1377676.1377685},
  doi          = {10.1145/1377676.1377685},
  acmid        = {1377685},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  keywords     = {johnson-lindenstrauss random projection},
}

@article{blum2006random,
  author       = {Blum, Avrim},
  doi          = {10.1007/11752790_3},
  isbn         = {3-540-34137-4},
  issn         = {16113349},
  journal      = {Subspace, Latent Structure and Feature Selection},
mendeley-groups = {Random matrices},
  pages        = {52--68},
  publisher    = {Springer},
  title        = {{Random projection, margins, kernels, and feature-selection}},
  url          = {{\textless}Go to ISI{\textgreater}://WOS:000238094700003},
  volume       = {3940},
  year         = {2006},
}

@article{davenport2007smashed,
  abstract     = {The theory of compressive sensing (CS) enables the reconstruction of a sparse or compressible image or signal from a small set of linear, non-adaptive (even random) projections. However, in many applications, including object and target recognition, we are ultimately interested in making a decision about an image rather than computing a reconstruction. We propose here a framework for compressive classification that operates directly on the compressive measurements without first reconstructing the image. We dub the resulting dimensionally reduced matched filter the smashed filter. The first part of the theory maps traditional maximum likelihood hypothesis testing into the compressive domain; we find that the number of measurements required for a given classification performance level does not depend on the sparsity or compressibility of the images but only on the noise level. The second part of the theory applies the generalized maximum likelihood method to deal with unknown transformations such as the translation, scale, or viewing angle of a target object. We exploit the fact the set of transformed images forms a low-dimensional, nonlinear manifold in the high-dimensional image space. We find that the number of measurements required for a given classification performance level grows linearly in the dimensionality of the manifold but only logarithmically in the number of pixels/samples and image classes. Using both simulations and measurements from a new single-pixel compressive camera, we demonstrate the effectiveness of the smashed filter for target classification using very few measurements.},
  author       = {Davenport, Mark A and Duarte, Marco F and Wakin, Michael B and Laska, Jason N and Takhar, Dharmpal and Kelly, Kevin F and Baraniuk, Richard},
  doi          = {10.1117/12.714460},
  isbn         = {0001406108},
  issn         = {0277786X},
  journal      = {Proc. SPIE 6498, Computational Imaging V},
  keywords     = {compressive sensing,image classification,object recognition,smashed filter},
mendeley-groups = {Random matrices},
  month        = jan,
  pages        = {64980H--1--12},
  pmid         = {21685898},
  title        = {{The smashed filter for compressive classification and target recognition}},
  url          = {http://dx.doi.org/10.1117/12.714460$\backslash$nhttp://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1298717},
  volume       = {6498},
  year         = {2007},
}

@inproceedings{duarte2006sparse,
  author       = {Duarte, Marco F and Davenport, Mark A and Wakin, Michael B and Baraniuk, Richard G},
  booktitle    = {Acoustics, Speech and Signal Processing, ICASSP Proceedings.},
  doi          = {10.1109/ICASSP.2006.1660651},
  isbn         = {1520-6149},
  issn         = {1520-6149},
mendeley-groups = {Random matrices},
  organization = {IEEE},
  pages        = {305--308},
  title        = {{Sparse Signal Detection From Incoherent Projections}},
  volume       = {3},
  year         = {2006},
}

@inproceedings{duarte2007multiscale,
  abstract     = {We propose a framework for exploiting dimension-reducing random projections in detection and classification problems. Our approach is based on the generalized likelihood ratio test; in the case of image classification, it exploits the fact that a set of images of a fixed scene under varying articulation parameters forms a low-dimensional, nonlinear manifold. Exploiting recent results showing that random projections stably embed a smooth manifold in a lower-dimensional space, we develop the multiscale smashed filter as a compressive analog of the familiar matched filter classifier. In a practical target classification problem using a single-pixel camera that directly acquires compressive image projections, we achieve high classification rates using many fewer measurements than the dimensionality of the images.},
  author       = {Duarte, M F and Davenport, M A and Wakin, M B and Laska, J N and Takhar, D and Kelly, K F and Baraniuk, R G},
  booktitle    = {IEEE International Conference on Image Processing, 2007. ICIP 2007.},
  doi          = {10.1109/ICIP.2007.4379546},
  isbn         = {1522-4880},
  keywords     = {CS theory,Cameras,Classification algorithms,Image classification,Image coding,Image reconstruction,Instruments,Layout,Matched filters,Testing,Vectors,compressive image classification problem,data compression,filtering theory,generalized likelihood ratio test,image matching,multiscale dimension-reducing random projection,multiscale smashed filter classifier,object recognition,single-pixel camera,statistical testing},
mendeley-groups = {Random matrices},
  organization = {IEEE},
  pages        = {161--164},
  title        = {{Multiscale Random Projections for Compressive Classification}},
  volume       = {6},
  year         = {2007},
}

@inproceedings{haupt2006compressive,
  abstract     = {Compressive sampling (CS), also called compressed sensing, entails making observations of an unknown signal by projecting it onto random vectors. Recent theoretical results show that if the signal is sparse (or nearly sparse) in some basis, then with high probability such observations essentially encode the salient information in the signal. Further, the signal can be reconstructed from these "random projections," even when the number of observations is far less than the ambient signal dimension. The provable success of CS for signal reconstruction motivates the study of its potential in other applications. This paper investigates the utility of CS projection observations for signal classification (more specifically, m-ary hypothesis testing). Theoretical error bounds are derived and verified with several simulations.},
  author       = {Haupt, Jarvis and Castro, Rui and Nowak, Robert and Fudge, Gerald and Yeh, Alex},
  booktitle    = {2006 Fortieth Asilomar Conference on Signals, Systems and Computers},
  doi          = {10.1109/ACSSC.2006.354994},
  isbn         = {1-4244-0784-2},
  issn         = {10586393},
mendeley-groups = {Random matrices},
  organization = {IEEE},
  pages        = {1430--1434},
  title        = {{Compressive Sampling for Signal Classification}},
  url          = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=4176804{\&}contentType=Conference+Publications{\&}queryText=haupt+compressive+sensing},
  year         = {2006},
}

@inproceedings{hegde2007random,
  abstract     = {We propose a novel method for emphlinear dimensionality reduction of manifold modelled data. First we show that with a small number M of emphrandom projections of sample points in mathbbR N belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigourously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N, meaning that K {\textless} M ll N. To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.},
  author       = {Hegde, Chinmay and Wakin, Michael B},
  booktitle    = {Neural Information Processing Systems},
  isbn         = {9550071030},
mendeley-groups = {Random matrices},
  pages        = {641--648},
  title        = {{Random Projections for Manifold Learning}},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.2757{\&}rep=rep1{\&}type=pdf},
  volume       = {M},
  year         = {2007},
}
@inproceedings{indyk1998approximate,
  author       = {Indyk, P and Motwani, R},
  booktitle    = {Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  doi          = {10.4086/toc.2012.v008a014},
  isbn         = {0897919629},
  issn         = {00123692},
mendeley-groups = {Random matrices},
  organization = {ACM},
  pages        = {604--613},
  pmid         = {15486356},
  title        = {{Approximate nearest neighbors: towards removing the curse of dimensionality}},
  year         = {1998},
}
@article{zhou2009compressed,
  abstract     = {Recent research has studied the role of sparsity in high-dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models. This line of work shows that lscr1 -regularized least squares regression can accurately estimate a sparse linear model from noisy examples in high dimensions. We study a variant of this problem where the original n input variables are compressed by a random linear transformation to m Lt n examples in p dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of projections that are required for lscr1 -regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ldquosparsistence.rdquo We also show that lscr1 -regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ldquopersistence.rdquo Finally, we characterize the privacy properties of the compression procedure, establishing upper bounds on the mutual information between the compressed and uncompressed data that decay to zero.},
  author       = {Zhou, Shuheng and Lafferty, John and Wasserman, Larry},
  doi          = {10.1109/TIT.2008.2009605},
  isbn         = {0018-9448},
  issn         = {00189448},
  journal      = {IEEE Trans. Inf. Theory},
  keywords     = {???1 regularization,Capacity of multiple-antenna channels,Compressed sensing,High-dimensional regression,Lasso,Privacy,Sparsity},
mendeley-groups = {Random matrices},
  number       = {2},
  pages        = {846--866},
  publisher    = {IEEE},
  title        = {{Compressed and privacy-sensitive sparse regression}},
  volume       = {55},
  year         = {2009},
}

@article{candes2005decoding,
  abstract     = {This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f∈Rn from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the ℓ1-minimization problem (||x||ℓ1:=$\Sigma$i|xi|) min(g∈Rn) ||y - Ag||ℓ1 provided that the support of the vector of errors is not too large, ||e||ℓ0:=|{\{}i:ei ≠ 0{\}}|≤$\rho${\textperiodcentered}m for some $\rho${\textgreater}0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of ℓ1 is a crucial property we call the uniform uncertainty principle that we shall describe in detail.},
  archiveprefix= {arXiv},
  arxivid      = {math/0502327},
  author       = {Candes, Emmanuel. J. and Tao, Terence},
  doi          = {10.1109/TIT.2005.858979},
  eprint       = {0502327},
  file         = {:D$\backslash$:/Users/Subhy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Candes, Tao - 2005 - Decoding by Linear Programming.pdf:pdf},
  issn         = {0018-9448},
  journal      = {IEEE Trans. Inf. Theory},
  keywords     = {Basis pursuit,Decoding,Equations,Error correction,Error correction codes,Gaussian processes,Gaussian random matrices,Gaussian random matrix,Information theory,Linear code,Linear programming,Mathematics,Sparse matrices,Vectors,basis pursuit,convex programming,decoding,decoding of (random) linear codes,duality in optimization,error correction codes,indeterminancy,linear code decoding,linear codes,linear programming,minimisation,minimization problem,natural error correcting problem,principal angles,random codes,restricted orthonormality,simple convex optimization problem,singular values of random matrices,sparse matrices,sparse solution,sparse solutions to underdetermined systems,uncertainty principle},
mendeley-groups = {Random matrices},
  month        = dec,
  number       = {12},
  pages        = {4203--4215},
  primaryclass = {math.MG},
  shorttitle   = {IEEE Transactions on Information Theory},
  title        = {{Decoding by Linear Programming}},
  url          = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1542412 http://arxiv.org/abs/math/0502327},
  volume       = {51},
  year         = {2005},
}

@article{baraniuk2008simple,
  abstract     = {We give a simple technique for verifying the Restricted Isometry Property (as introduced by Cand{\`{e}}s and Tao) for random matrices that underlies Compressed Sensing. Our approach has two main ingredients: (i) concentration inequalities for random inner products that have recently provided algorithmically simple proofs of the Johnson–Lindenstrauss lemma; and (ii) covering numbers for finite-dimensional balls in Euclidean space. This leads to an elementary proof of the Restricted Isometry Property and brings out connections between Compressed Sensing and the Johnson–Lindenstrauss lemma. As a result, we obtain simple and direct proofs of Kashin's theorems on widths of finite balls in Euclidean space (and their improvements due to Gluskin) and proofs of the existence of optimal Compressed Sensing measurement matrices. In the process, we also prove that these measurements have a certain universality with respect to the sparsity-inducing basis.},
  author       = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  doi          = {10.1007/s00365-007-9003-x},
  isbn         = {0176427614320940},
  issn         = {01764276},
  journal      = {Constructive Approximation},
  keywords     = {Compressed sensing,Concentration inequalities,Random matrices,Sampling},
mendeley-groups = {Random matrices},
  number       = {3},
  pages        = {253--263},
  publisher    = {Springer},
  title        = {{A simple proof of the restricted isometry property for random matrices}},
  volume       = {28},
  year         = {2008},
}

@article{advani2013statistical,
  abstract     = {Recent experimental advances in neuroscience have opened new vistas into the immense complexity of neuronal networks. This proliferation of data challenges us on two parallel fronts. First, how can we form adequate theoretical frameworks for understanding how dynamical network processes cooperate across widely disparate spatiotemporal scales to solve important computational problems? And second, how can we extract meaningful models of neuronal systems from high dimensional datasets? To aid in these challenges, we give a pedagogical review of a collection of ideas and theoretical methods arising at the intersection of statistical physics, computer science and neurobiology. We introduce the interrelated replica and cavity methods, which originated in statistical physics as powerful ways to quantitatively analyze large highly heterogeneous systems of many interacting degrees of freedom. We also introduce the closely related notion of message passing in graphical models, which originated in computer science as a distributed algorithm capable of solving large inference and optimization problems involving many coupled variables. We then show how both the statistical physics and computer science perspectives can be applied in a wide diversity of contexts to problems arising in theoretical neuroscience and data analysis. Along the way we discuss spin glasses, learning theory, illusions of structure in noise, random matrices, dimensionality reduction, and compressed sensing, all within the unified formalism of the replica method. Moreover, we review recent conceptual connections between message passing in graphical models, and neural computation and learning. Overall, these ideas illustrate how statistical physics and computer science might provide a lens through which we can uncover emergent computational functions buried deep within the dynamical complexities of neuronal networks.},
  primaryclass = {q-bio.NC},
  archiveprefix= {arXiv},
  arxivid      = {1301.7115},
  author       = {Advani, Madhu and Lahiri, Subhaneil and Ganguli, Surya},
  doi          = {10.1088/1742-5468/2013/03/P03014},
  eprint       = {1301.7115},
  file         = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Review paper/Paper/Advani, Lahiri, Ganguli - 2013 - Statistical mechanics of complex neural systems and high dimensional data(2).pdf:pdf},
  issn         = {1742-5468},
  journal      = {J. Stat. Mech.},
  keywords     = {cavity method,glasses,high dimensional data,learning,message passing,neural networks,random matrices,random projections,replica method,spin},
mendeley-groups = {Random matrices},
  month        = jan,
  number       = {03},
  pages        = {P03014},
  title        = {{Statistical mechanics of complex neural systems and high dimensional data}},
  url          = {http://stacks.iop.org/1742-5468/2013/i=03/a=P03014 http://stacks.iop.org/1742-5468/2013/i=03/a=P03014?key=crossref.5f17f6bb3ee4eb696f3064bf1d40ac56 http://iopscience.iop.org/1742-5468/2013/03/P03014 http://arxiv.org/abs/1301.7115 http://dx.doi.org/10.1088},
  volume       = {2013},
  year         = {2013},
}

@article{simplicitycomp15,
  abstract     = {Technological advances have dramatically expanded our ability to probe multi-neuronal dynamics and connectivity in the brain. However, our ability to extract a simple conceptual understanding from complex data is increasingly hampered by the lack of theoretically principled data analytic procedures, as well as theoretical frameworks for how circuit connectivity and dynamics can conspire to generate emergent behavioral and cognitive functions. We review and outline potential avenues for progress, including new theories of high dimensional data analysis, the need to analyze complex artificial networks, and methods for analyzing entire spaces of circuit models, rather than one model at a time. Such interplay between experiments, data analysis and theory will be indispensable in catalyzing conceptual advances in the age of large-scale neuroscience.},
  archiveprefix= {arXiv},
  primaryclass = {q-bio.NC},
  arxivid      = {1503.08779},
  author       = {Gao, Peiran and Ganguli, Surya},
  doi          = {10.1016/j.conb.2015.04.003},
  eprint       = {1503.08779},
  journal      = {Current Opinion in Neurobiology},
mendeley-groups = {Random matrices},
  pages        = {148--155},
  pmid         = {25932978},
  isbn         = {0959-4388},
  issn         = {18736882},
  title        = {{On simplicity and complexity in the brave new world of large-scale neuroscience}},
  volume       = {32},
  year         = {2015},
}

@article{ganguli2012annrevs,
  abstract     = {The curse of dimensionality poses severe challenges to both technical and conceptual progress in neuroscience. In particular, it plagues our ability to acquire, process, and model high-dimensional data sets. Moreover, neural systems must cope with the challenge of processing data in high dimensions to learn and operate successfully within a complex world. We review recent mathematical advances that provide ways to combat dimensionality in specific situations. These advances shed light on two dual questions in neuroscience. First, how can we as neuroscientists rapidly acquire high-dimensional data from the brain and subsequently extract meaningful models from limited amounts of these data? And second, how do brains themselves process information in their intrinsically high-dimensional patterns of neural activity as well as learn meaningful, generalizable models of the external world from limited experience?},
  author       = {Ganguli, Surya and Sompolinsky, Haim},
  doi          = {10.1146/annurev-neuro-062111-150410},
  journal      = {Annual Review of Neuroscience},
  keywords     = {Brain,Brain: physiology,Data Interpretation,Humans,Learning,Learning: physiology,Memory,Mental Processes,Mental Processes: physiology,Models,Neural Networks (Computer),Neurons,Neurons: physiology,Nonlinear Dynamics,Short-Term,Short-Term: physiology,Statistical},
mendeley-groups = {Random matrices},
  number       = {1},
  pages        = {485--508},
  url          = {http://www.ncbi.nlm.nih.gov/pubmed/22483042},
  isbn         = {1545-4126 (Electronic) 0147-006X (Linking)},
  issn         = {0147-006X},
  pmid         = {22483042},
  title        = {{Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis}},
  volume       = {35},
  year         = {2012},
}

@article{amelunxen2014living,
  abstract     = {Recent research indicates that many convex optimization problems with random constraints exhibit a phase transition as the number of constraints increases. For example, this phenomenon emerges in the {$ell_1$} minimization method for identifying a sparse vector from random linear measurements. Indeed, the {$\ell_1$} approach succeeds with high probability when the number of measurements exceeds a threshold that depends on the sparsity level; otherwise, it fails with high probability. This paper provides the first rigorous analysis that explains why phase transitions are ubiquitous in random convex optimization problems. It also describes tools for making reliable predictions about the quantitative aspects of the transition, including the location and the width of the transition region. These techniques apply to regularized linear inverse problems with random measurements, to demixing problems under a random incoherence model, and also to cone programs with random affine constraints. The applied results depend on foundational research in conic geometry. This paper introduces a summary parameter, called the statistical dimension, that canonically extends the dimension of a linear subspace to the class of convex cones. The main technical result demonstrates that the sequence of intrinsic volumes of a convex cone concentrates sharply around the statistical dimension. This fact leads to accurate bounds on the probability that a randomly rotated cone shares a ray with a fixed cone.},
  archiveprefix= {arXiv},
  arxivid      = {1303.6672},
  primaryclass = {cs.IT},
  author       = {Amelunxen, Dennis and Lotz, Martin and McCoy, Michael B and Tropp, Joel A},
  doi          = {10.1093/imaiai/iau005},
  eprint       = {1303.6672},
  journal      = {Information and Inference},
mendeley-groups = {Random matrices},
  month        = apr,
  pages        = {1--52},
  url          = {http://users.cms.caltech.edu/~jtropp/papers/ALMT14-Living-Edge-preprint.pdf},
  issn         = {2049-8764},
  publisher    = {Oxford University Press},
  title        = {{Living on the edge: Phase transitions in convex programs with random data}},
  volume       = {1},
  year         = {2014},
}

@article{oymak2015universality,
  abstract     = {Dimension reduction is the process of embedding high-dimensional data into a lower dimensional space to facilitate its analysis. In the Euclidean setting, one fundamental technique for dimension reduction is to apply a random linear map to the data. This dimension reduction procedure succeeds when it preserves certain geometric features of the set. The question is how large the embedding dimension must be to ensure that randomized dimension reduction succeeds with high probability. This paper studies a natural family of randomized dimension reduction maps and a large class of data sets. It proves that there is a phase transition in the success probability of the dimension reduction map as the embedding dimension increases. For a given data set, the location of the phase transition is the same for all maps in this family. Furthermore, each map has the same stability properties, as quantified through the restricted minimum singular value. These results can be viewed as new universality laws in high-dimensional stochastic geometry. Universality laws for randomized dimension reduction have many applications in applied mathematics, signal processing, and statistics. They yield design principles for numerical linear algebra algorithms, for compressed sensing measurement ensembles, and for random linear codes. Furthermore, these results have implications for the performance of statistical estimation methods under a large class of random experimental designs.},
  archiveprefix= {arXiv},
  primaryclass = {math.PR},
  arxivid      = {1511.09433},
  author       = {Oymak, Samet and Tropp, Joel A},
  eprint       = {1511.09433},
  keywords     = {and phrases,code,conic geometry,convex geometry,dimension reduction,invariance principle,limit theorem,random,random matrix,randomized numerical linear algebra,signal reconstruction,statistical estimation,stochastic geometry},
mendeley-groups = {Random matrices},
  month        = nov,
  title        = {{Universality laws for randomized dimension reduction, with applications}},
  url          = {http://arxiv.org/abs/1511.09433},
  year         = {2015},
}

@book{pinkus2012n,
  author       = {Pinkus, Allan},
  doi          = {10.1007/978-3-642-69894-1},
mendeley-groups = {Random matrices},
  url          = {http://opac.inria.fr/record=b1092277 http://dx.doi.org/10.1007/978-3-642-69894-1},
  isbn         = {3-540-13638-X},
  pages        = {291},
  publisher    = {Springer},
  title        = {{$n$}-widths in approximation theory},
  volume       = {7},
  year         = {1985},
}

@article{wishart1928generalised,
  author       = {Wishart, J},
  doi          = {10.1017/S0305004100011063},
  journal      = {Biometrika},
mendeley-groups = {Random matrices},
  url          = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Generalised+Product+Moment+Distribution+in+Samples+from+a+Normal+Multivariate+Population{\#}0},
  issn         = {0305-0041},
  number       = {1},
  pages        = {32--52},
  publisher    = {JSTOR},
  title        = {{The generalised product moment distribution in samples from a normal multivariate population}},
  volume       = {20},
  year         = {1928},
}

@techreport{verma2011note,
  abstract     = {Random projections are typically used to study low distortion linear embeddings that approximately preserve Euclidean distances between pairs of points in a set S in R{\^{}}D . Of particular interest is when the set S is a low-dimensional submanifold of R{\^{}}D . Recent results by Baraniuk and Wakin [2007] and Clarkson [2007] shed light on how to pick the projection dimension to achieve low distortion of Euclidean distances between points on a manifold. While preserving ambient Euclidean distances on a manifold does imply preserving intrinsic path-lengths between pairs of points on a manifold, here we investigate how one can reason directly about preserving path-lengths without having to appeal to the ambient Euclidean distances between points. In doing so, we can improve upon Baraniuk and Wakin's result by removing the dependence on the ambient dimension D, and simplify Clarkson's result by using a single covering quantity and giving explicit dependence on constants.},
  author       = {Verma, Nakul},
  booktitle    = {UC San Diego, Tech. Report CS2011-0971},
  file         = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Verma - 2011 - A note on random projections for preserving paths on a manifold.pdf:pdf},
  institution  = {Tech. Report CS2011-0971},
  keywords     = {manifold learning,random projections},
mendeley-groups = {Random matrices},
  pages        = {1--6},
  title        = {{A note on random projections for preserving paths on a manifold}},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.9077},
  year         = {2011},
}

@article{lahiri2016randproj,
  abstract     = {A ubiquitous phenomenon is that interesting signals or data concentrate on low dimensional smooth manifolds inside a high dimensional ambient Euclidean space. Random projections are a simple and powerful tool for dimensionality reduction of such signals and data. Previous, seminal works have studied bounds on how the number of projections needed to preserve the geometry of these manifolds, at a given accuracy, scales with the intrinsic dimensionality, volume and curvature of the manifold. However, such works employ definitions of volume and curvature that are inherently difficult to compute. Therefore such theory cannot be easily tested against numerical simulations to quantitatively understand the tightness of the proven bounds. We instead study the typical distortions arising in random projections of an ensemble of smooth Gaussian random manifolds. In doing so, we find explicitly computable, approximate theoretical bounds on the number of projections required to preserve the geometric structure of these manifolds to a prescribed level of accuracy. Our bounds, while approximate, can only be violated with a probability that is exponentially small in the ambient dimension, and therefore they hold with high probability in most cases of practical interest. Moreover, unlike previous work, we test our theoretical bounds against numerical experiments on the actual geometric distortions that typically occur for random projections of random smooth manifolds. Through this comparison, we find our bounds are tighter than previous results by several orders of magnitude.},
  primaryclass = {stat.ML},
  archiveprefix= {arXiv},
  arxivid      = {1607.04331},
  author       = {Lahiri, Subhaneil and Gao, Peiran and Ganguli, Surya},
  eprint       = {1607.04331},
  keywords     = {Computer Science - Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  month        = jul,
  title        = {{Random projections of random manifolds}},
  url          = {http://arxiv.org/abs/1607.04331},
  year         = {2016},
}

@article{poole2016deepchaos,
  abstract     = {We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.},
  primaryclass = {stat.ML},
  archiveprefix= {arXiv},
  arxivid      = {1606.05340},
  author       = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  eprint       = {1606.05340},
  month        = jun,
  title        = {{Exponential expressivity in deep neural networks through transient chaos}},
  url          = {http://arxiv.org/abs/1606.05340},
  year         = {2016},
}

@article{Eftekhari2015manifoldCS,
  abstract     = {Abstract Compressive Sensing (CS) exploits the surprising fact that the information contained in a sparse signal can be preserved in a small number of compressive, often random linear measurements of that signal. Strong theoretical guarantees have been established concerning the embedding of a sparse signal family under a random measurement operator and on the accuracy to which sparse signals can be recovered from noisy compressive measurements. In this paper, we address similar questions in the context of a different modeling framework. Instead of sparse models, we focus on the broad class of manifold models, which can arise in both parametric and non-parametric signal families. Using tools from the theory of empirical processes, we improve upon previous results concerning the embedding of low-dimensional manifolds under random measurement operators. We also establish both deterministic and probabilistic instance-optimal bounds in ℓ{\textless}inf{\textgreater}2{\textless}/inf{\textgreater} for manifold-based signal recovery and parameter estimation from noisy compressive measurements. In line with analogous results for sparsity-based CS, we conclude that much stronger bounds are possible in the probabilistic setting. Our work supports the growing evidence that manifold-based models can be used with high accuracy in compressive signal processing.},
  archiveprefix= {arXiv},
  arxivid      = {arXiv:1306.4748v1},
  author       = {Eftekhari, Armin and Wakin, Michael B.},
  doi          = {10.1016/j.acha.2014.08.005},
  eprint       = {arXiv:1306.4748v1},
  file         = {:D$\backslash$:/Users/Subhy/Documents/Neuro projects/Manifolds/Eftekhari, Wakin - 2015 - New analysis of manifold embeddings and signal recovery from compressive measurements.pdf:pdf},
  journal      = {Appl. Comput. Harmon. Anal.},
  keywords     = {Compressive sensing,Dimensionality reduction,Manifold embeddings,Manifolds,Parameter estimation,Random projections,Signal recovery},
mendeley-groups = {Random matrices},
  number       = {1},
  pages        = {67--109},
  title        = {{New analysis of manifold embeddings and signal recovery from compressive measurements}},
  volume       = {39},
  year         = {2015},
}
